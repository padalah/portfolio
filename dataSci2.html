<!DOCTYPE HTML>

<html>
	<head>
		<title>Dopetrope by HTML5 UP</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="homepage is-preload">
		<div id="page-wrapper">
			<div class="topnav" id="myTopnav">
				<a href="index.html" class="active">Home</a>
				<div class="dropdown">
				  <button class="dropbtn">Projects<i class="fa fa-caret-down"></i></button>
				  <div class="dropdown-content">
					<a href="airlines.html">Predicting Flight Delays</a>
					<a href="nlp.html">Exploring Catastrophic Forgetting for Question Answering Domain
						Adaptation in Text-to-Text Transformers</a>
					<a href="experiment.html">Effect of Political Ideology on Consumer Behavior</a>
					<a href="dataSci2.html">Pulmonary Nodule Segmentation</a>
					<a href="dataSci1.html">Poverty and COVID-19</a>
					<a href="genderMag.html">Barriers to entry in Open Source Software</a>
					<a href="pmWork.html">Certifications and Discounts</a>
					<a href="capstone.html">Interactive Wheel Based Image Recoloring GUI</a>
					<a href="ursa.html">Coreless Promoters in Arabadopsis DNA</a>
					<a href="health.html">Health Portal - Built for All</a>
				  </div>
				</div> 
			</div>
		<!--Intro -->
			<section id="main" class="box">
				<div id="title" class="container">
					<header class="major">
						<h2>Pulmonary Nodule Segmentation</h2>
					</header>
				</div>
					
				<div id="purpose" class="container">
					<h3>The Purpose</h3>
					<p>
						Our purpose was given the CT scans of a lung we want to predict the location(s) of the nodules in the lung. Currently, identification and classification of lung nodules are done by 
						experienced annotators. We aim to build a model that can segment a lung CT scan and predict the existence of a nodule.
					</p>
					<p>
						The dataset we are using is from a public dataset called Lung Nodule Analaysis 2016 (LUNA-16). It has a total of 888 CT scans. 
						There are a total of 1186 nodules for 601 of those CT scans. The annotations file contatins the x,y,z coordinates of each nodule with a size >= 3mm and its corresponding CT scan. The data files take up approximately 115 GB of data.
					</p>
					<p></p>
				</div>	

							
				<div id="quest" class = "container">
					<h3>The Quest</h3>
					<p>
						<ul>
							<li>Locate the nodules to use as input</li>
							<ul>
								<li>Using the annotated data, we know for each CT Scan where the nodules in that image are.</li>
								<li>Identify the nodules by matching the annotated locations given by the experts to the images.</li>											
								<li>A subset of these images (training set) will be processed so that a nodule mask will be generated for each nodule.</li>
								<li>The nodule masks will be inputted into the model.</li>
							</ul>
							<li>Classify each pixel in image (Semantic Segmentation)</li>	
							<ul>
								<li>Input CT scans into the model so the model can be trained.</li>
								<li>Models will classify each pixel in the image and label it as a nodule or not.</li>											
							</ul>
							<li>Once model is built we can use it to predict the location of lung nodules in other CT scans of lungs (testing set). </li>
						</ul>
					</p>
					<p></p>
				</div>
				
				<div id="solution" class = "container">
					<h3>The Solution</h3>
					<h4>Semantic Segmentation</h4>
					<figcaption>Semantic Segmentation is the process of taking an image and labeling each pixel as a type of object. In our case, the object will either be a nodule or not a nodule</figcaption>
					<a href="images/semanticSegmentation.png"><img id=bodyImg class="img-fluid" src="images/semanticSegmenatation.png" alt="" style="width:100%;max-width:900px"></a>
					<h4>Understanding our Dataset</h4>
					<figcaption>Nodules labeled by expert annotators on the lung CT scans</figcaption>
					<a href="images/nodulesHistogram2.png"><img id=bodyImg class="img-fluid" src="images/nodulesHistogram2.png" alt="" style="width:100%;max-width:900px"></a>
					<figcaption>Nodules distribution by size and frequency</figcaption>
					<a href="images/nodulesSize.png"><img id=bodyImg class="img-fluid" src="images/nodulesSize.png" alt="" style="width:100%;max-width:900px"></a>
					<h4>Preprocessing Images</h4>
					<figcaption>To identify lung tissue create a lung mask (Since lungs are full of air it is less dense in CT scans)</figcaption>
					<a href="images/lungMask.png"><img id=bodyImg class="img-fluid" src="images/lungMask.png" alt="" style="width:100%;max-width:900px"></a>
					<figcaption>To reduce size and noise of data identify a "region of interest" (i.e. the lungs) for the model to focus on</figcaption>
					<a href="images/regionOfInterest.png"><img id=bodyImg class="img-fluid" src="images/regionOfInterest.png" alt="" style="width:100%;max-width:900px"></a>
					<h4>Modeling and Training</h4>
					<p>
						We used a "U-Net" architecture for our Convolutional Neural Network (CNN) which consists of 2 branches, contracting 
						and expanding.
					</p>
					<figcaption>Semantic Segmentation is the process of taking an image and labeling each pixel as a type of object. In our case, the object will either be a nodule or not a nodule</figcaption>
					<a href="images/u-net.png"><img id=bodyImg class="img-fluid" src="images/u-net.png" alt="" style="width:100%;max-width:900px"></a>
					<figcaption>The base model parameters we built using the U-Net architecture were as follows:</figcaption>
					<a href="images/u-netParams.png"><img id=bodyImg class="img-fluid" src="images/u-netParams.png" alt="" style="width:100%;max-width:900px"></a>
					<p>
						We used tensorflow_cloud api to train the model in Google AI Platform. 
					</p>
					<p></p>
				</div>
				
				<div id="impact" class = "container">
					<h3>The Impact</h3>
					<figcaption>Performance of the Base Model (the training data had a dice coefficent of 0.799 and the validation data had a dice coefficent of 0.690. </figcaption>
					<a href="images/baseModelPerf.png"><img id=bodyImg class="img-fluid" src="images/baseModelPerf.png" alt=""></a>
					<figcaption>The base model parameters we built using the U-Net architecture were as follows:</figcaption>
					<a href="images/baseModelOptimizations.png"><img id=bodyImg class="img-fluid" src="images/baseModelOptimizations.png" alt=""></a>
					<figcaption>Here a subset of predicted images next to annotated images:</figcaption>
					<a href="images/baseModelOptimizations.png"><img id=bodyImg class="img-fluid" src="images/baseModelOptimizations.png" alt=""></a>
					<p></p>
				</div>
			</section>		

			<!-- Footer -->
			<section id="footer" class="box">
				<div id="footer" class="container">
					<div class="row">
						<div class="col-12">
							<section>
								<ul class="social">
									<li><a class="icon brands fa-linkedin-in" href="https://www.linkedin.com/in/susmitapadala/"><span class="label">LinkedIn</span></a></li>
									<li><a class="icon brands fa-github" href="https://github.com/padalah"><span class="label">Github</span></a></li>
								</ul>
							</section>
							<!-- Copyright -->
							<div id="copyright">
								<ul class="links">
									<li>&copy; Susmita Padala. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
								</ul>
							</div>

						</div>
					</div>
				</div>
			</section>

		</div>

		<!-- Scripts -->
		<script src="assets/js/jquery.min.js"></script>
		<script src="assets/js/jquery.dropotron.min.js"></script>
		<script src="assets/js/browser.min.js"></script>
		<script src="assets/js/breakpoints.min.js"></script>
		<script src="assets/js/util.js"></script>
		<script src="assets/js/main.js"></script>

	</body>
</html>